# ------------------------------------ USAGE -----------------------------------
# The YAML configuration contains all necessary paths and parameters to download
# data, train a tokenizer, tokenize the data, train a model, and evaluate the
# model.
#
# To use a YAML config, create a copy of template_config.yaml in the
# user_configs folder and fill in the necessary parameters.
#
# Path names need to be specified properly and be absolute paths. A suggested
# path structure is given in the template. The YAML config file is then passed
# as an argument to any scripts.
#
# For example, to train a model, run the following in the repository root
# directory:
#        python3 train_model.py ./configs/user_configs/my_config.yaml
#
# Do not Add a YAML config to a git commit unless you have a good reason.

# ---------------------------- DATASET CONFIGURATION ---------------------------

# HuggingFace Dataset Feature (str): Column of the dataset to extract text from
dataset_feature: "text"
# HuggingFace Dataset Name (str)
dataset_name: "wikitext"
# HuggingFace Dataset Subset (str)
dataset_subset: "wikitext-2-v1"

# Root Data Path: Useful for any new features that don't want to specify a new
# path parameter in the config
root_data_path: "/YOUR/PATH/HERE/data"

# Download Data: Path to raw dataset folder
raw_dataset_path: "/YOUR/PATH/HERE/data/datasets/wikitext"

# Train Tokenizer: Path to tokenizer folder
tokenizer_path: "/YOUR/PATH/HERE/data/tokenizers/wikitext_tokenizer"

# Tokenize Data: Path to tokenized dataset folder
tokenized_dataset_path: "/YOUR/PATH/HERE/data/tokenized_dataset/wikitext"

# Train Model: Path to model folder
models_path: "/YOUR/PATH/HERE/data/models"

# TensorBoard Path
tboard_path: ~

# Splits: Train split, Validation Split, and Test Split
splits:
  - 0.7
  - 0.2
  - 0.1

# ------------------------ MODEL TRAINING CONFIGURATION ------------------------

# Activation Dropout (float): Probability of an element to be zeroed during
# dropout after activation between FFN layers
activation_dropout: 0.0
# Batch Size (int): Batch size for training
batch_size: 8
# Checkpoints (bool): Whether to save checkpoints during training
checkpoints: false
# Device Configuration (str)
device: "cuda" # Choices: "cpu", "cuda"
# Dropout (float): Probability of an element to be zeroed during dropout
dropout: 0.1
# Embedding Dimension (int): Embedding dimension size of each token
embed_dim: 80
# Epochs (int): Number of epochs to train for
epochs: 1
# FFN Dimension (int): Hidden layer size of Feed Forward Network (FFN)
ffn_dim: 12
# Fully Sharded Data Parallel (bool): Whether to shard Module parameters across
# data parallel workers or not (with the FairScale library)
fsdp: false
# Heads (int): Number of heads. Head architecture changes based on model
heads: 4
# Layers (int): Number of stacked network layers
layers: 2
# Learning Rate (float)
learning_rate: 0.001
# Model Type (str): Name of model architecture to train
model_type: retnet # Choices: "retnet", "transformer"
# Number of GPUs (int)
num_devices: 1
# Random Seed (int): For reproducibility
rand_seed: 42
# Sequence Length (int): Context window size by number of tokens
seq_len: 128
# Validation Frequency (int): Number of times to validate per epoch
validation_frequency: 3
# Value Embedding Dimension (int): Value embed dimension size
value_embed_dim: 12
# Vocabulary Size (int): Maximum vocabulary size (unique tokens in vocabulary)
vocab_size: 4000
