# ------------------------------------ USAGE -----------------------------------
# The YAML configuration contains all necessary paths and parameters to download
# data, train a tokenizer, tokenize the data, train a model, and evaluate the
# model.
#
# To use a YAML config, create a copy of template_config.yaml in the
# user_configs folder and fill in the necessary parameters.
#
# Path names need to be specified properly and be absolute paths. A suggested
# path structure is given in the template. The YAML config file is then passed
# as an argument to any scripts.
#
# For example, to train a model, run the following in the repository root
# directory:
#        python3 train_model.py ./configs/user_configs/my_config.yaml
#
# Do not Add a YAML config to a git commit unless you have a good reason.

# ----------------------- DATASET AND PATH CONFIGURATION -----------------------

# Checkpoint Path: path to checkpoint to load for generation/inference
checkpoint_path: ~

# HuggingFace Dataset Feature (str): Column of the dataset to extract text from
dataset_feature: "text"
# HuggingFace Dataset Name (str)
dataset_name: "wikitext"
# HuggingFace Dataset Subset (str)
dataset_subset: "wikitext-2-v1"

# Train Model: Path to model folder
models_path: "/YOUR/PATH/HERE/data/models"

# Root Data Path: Useful for any new features that don't want to specify a new
# path parameter in the config
root_data_path: "/YOUR/PATH/HERE/data"

# Download Data: Path to raw dataset folder
raw_dataset_path: "/YOUR/PATH/HERE/data/datasets/wikitext"

# Splits: Train split, Validation Split, and Test Split
splits:
  - 0.7
  - 0.2
  - 0.1

# Train Tokenizer: Path to tokenizer folder
tokenizer_path: "/YOUR/PATH/HERE/data/tokenizers/wikitext_tokenizer"

# Tokenize Data: Path to tokenized dataset folder
tokenized_dataset_path: "/YOUR/PATH/HERE/data/tokenized_dataset/wikitext"

# TensorBoard Path, optional
tboard_path: "/YOUR/PATH/HERE/data/runs"

# ---------------------------- DEVICE CONFIGURATION ----------------------------

# Device Type (str): Device to train on
device: "cuda" # Options: "cpu", "cuda"
# Number of GPUs (int)
num_devices: 1
# Number of Nodes (int)
num_nodes: 1
# Strategy (string): Distributed strategy for training. Likely no need to change
strategy: "ddp" # Options: "ddp", "ddp_spawn"
# Use Slurm (bool): Whether to use Slurm for training
use_slurm: true

# --------------------------- TRAINING CONFIGURATION ---------------------------

# Accumulate Gradient Batches (int): Accumulate gradients over n batches
accumulate_grad_batches: 1
# Epochs (int): Number of epochs to train for
epochs: 1
# Gamma (float): Learning rate scheduler gamma
gamma: 0.85
# Learning Rate (float): Learning rate of model to train
learning_rate: 0.001
# Random Seed (int): Random seed for reproducibility
rand_seed: 42
# Save Top K (int): Number of best models to save
save_top_k: 3
# Validation Check Interval (float): Validation frequency (fraction of an epoch)
val_check_interval: 0.5

# ---------------------------- MODEL CONFIGURATION -----------------------------

# Activation Dropout (float): Probability of an element to be zeroed during
# dropout after activation between FFN layers
activation_dropout: 0.0
# Batch Size (int): Batch size for training
batch_size: 8
# Checkpoints (bool): Whether to save checkpoints during training
checkpoints: false
# Dropout (float): Probability of an element to be zeroed during dropout
dropout: 0.1
# Embedding Dimension (int): Embedding dimension size of each token
embed_dim: 80
# FFN Dimension (int): Hidden layer size of Feed Forward Network (FFN)
ffn_dim: 12
# Heads (int): Number of heads. Head architecture changes based on model
heads: 4
# Layers (int): Number of stacked network layers
layers: 2
# Model Type (str): Name of model architecture to train
model_type: "retnet" # Choices: "retnet", "transformer"
# Sequence Length (int): Context window size by number of tokens
seq_len: 128
# Value Embedding Dimension (int): Value embed dimension size
value_embed_dim: 12
# Vocabulary Size (int): Maximum vocabulary size (unique tokens in vocabulary)
vocab_size: 4000
