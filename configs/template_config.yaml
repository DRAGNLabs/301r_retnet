# USAGE: The YAML configuration contains all necessary paths and parameters to download data
#        train a tokenizer, tokenize the data, train a model, and evaluate the model.
#
#        To prepare a YAML config for usage, create a copy of template_config.yaml 
#        in the user_configs folder and fill in the necessary parameters.
#        Path names need to be specified properly. These should be absolute paths.
#        A suggestion for path structure is given in the template.
#        The YAML config file is then passed as an argument to any scripts.
#
#        For example, to train a model, run:
#        python train_model.py ./configs/user_configs/my_config.yaml
#
#        Do not add a YAML config to a git commit unless you have a good reason.

# Dataset Configuration

# HuggingFace Dataset Feature
dataset_feature: "text" 
# HuggingFace Dataset Name
dataset_name: "wikitext" 
# HuggingFace Dataset Subset
dataset_subset: "wikitext-2-v1" 

# Root Data Path
# This path is useful for any new features that don't want to specify a new path parameter in the config
root_data_path: "/YOUR/PATH/HERE/data"

# Download Data
# Path to raw dataset folder
raw_dataset_path: "/YOUR/PATH/HERE/data/datasets/wikitext" 

# Train Tokenizer
# Path to tokenizer folder
tokenizer_path: "/YOUR/PATH/HERE/data/tokenizers/wikitext_tokenizer"

# Tokenize Data
# Path to tokenized dataset folder
tokenized_dataset_path: "/YOUR/PATH/HERE/data/tokenized_dataset/wikitext"

# Train Model
# Path to model folder
models_path: "/YOUR/PATH/HERE/data/models"

# Tensorboard path
tboard_path: ~

# Splits
# Fraction Train, Fraction Validation, Fraction Test
splits:
  - 0.7
  - 0.2
  - 0.1

# Device Configuration
device: "cuda" # Options: "cpu", "cuda"
# num_nodes (int): Number of nodes
num_nodes: 1
# Number of GPUs (int)
num_devices: 1 
# strategy (string): Distributed strategy for training. You probably don't need to change this
strategy: "ddp" # Options: "ddp", "ddp_spawn". 

# Training Configuration
rand_seed: 42
# epochs (int): Number of epochs to train for.
epochs: 1
# validation_frequency (int): How often to validate per epoch
validation_frequency: 3
# lr (float): Learning rate of model to train.
learning_rate: 0.001 # lr
# save_top_k (int): Number of best models to save.
save_top_k: 3
# val_check_interval (float): How often to validate as a fraction of an epoch.
val_check_interval: 0.5
# use_slurm (bool): Whether to use SLURM for training.
use_slurm: true

# Model Trainer Configuration
# Model Parameters

# accumulate_grad_batches (int): Accumulate gradients over n batches.
accumulate_grad_batches: 1
# activation_dropout (float): Probability of an element to be zeroed during dropout after activation between FFN layers.
activation_dropout: 0.0
# attention_heads (int): Number of attention heads in MHA module.
attention_heads: 2
# batch_size (int): Batch size for training.
batch_size: 8
# checkpoints (bool): Whether to save checkpoints during training.
checkpoints: false
# dropout (float): Probability of an element to be zeroed during dropout.
dropout: 0.1
# embed_dim (int): Embedding dimension size of each token.
embed_dim: 80
# ffn_dim (int): Hidden layer size of Feed Forward Network (FFN).
ffn_dim: 12
#fsdp (bool): Whether to shard Module parameters across data parallel workers or not (with the FairScale library).
fsdp: false
# layers (int): Number of retention network layers.
layers: 2
# heads (int): Number of heads. Head architecture changes based on model.
heads: 4 # TODO: is this parameter obselete now?
# model_type (str): Name of model architecture to train.
model_type: retnet # Choices: "retnet", "transformer"
# retention_heads (int): Number of retention heads in MSR module.
retention_heads: 2
# seq_len (int): Sequence length (context window size).
seq_len: 128
# value_embed_dim (int): Value embed dimension size.
value_embed_dim: 12
# vocab_size (int): Maximum vocabulary size (number of unique tokens in vocabulary)
vocab_size: 4000
