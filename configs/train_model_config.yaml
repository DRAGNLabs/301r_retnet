# Model Trainer Configuration

# Model Parameters
activation_dropout: 0.0
batch_size: 32
checkpoints: false
dropout: 0.1
embed_dim: 768
epochs: 10
ffn_dim: 1280
fsdp: false
layers: 12
learning_rate: 0.001 # lr
model_type: retnet # Choices: "retnet", "transformer"
heads: 3
seq_len: 512
value_embed_dim: 1280
vocab_size: 4000

# Dataset Configuration
dataset_feature: "text"
dataset_name: "wikitext"
dataset_subset: "wikitext-2-v1"
raw_dataset_dir: "/Users/jtappen/301R/301r_retnet/scripts/data/wikitext"

#Train Tokenizer
tokenizer_path: "/Users/jtappen/CS301R/301r_retnet/data/tokenizers/wikitext_tokenizer"

#Tokenize Data
tokenized_data_name: "wikitext-2-v1-tokenized"
tokenized_data_dir: "/Users/jtappen/301R/301r_retnet/scripts/data/wikitext/tokenized_data_wikitext"

#Train Model
data_dir: "/Users/jtappen/301R/301r_retnet/scripts/data/models"

splits:
  - 0.7
  - 0.2
  - 0.1

# Paths and Directories
data_directory: "/tmp/data" # data-dir
dataset_directory: "/tmp/data/dataset" # dataset-dir
tensorboard_directory: "/tmp/data/logs" # tboard-dir

# Device Configuration
device: "cuda" # Options: "cpu", "cuda"

# Training Configuration
rand_seed: 42 # rand-seed
validation_frequency: 3 # val-freq
