# Model Trainer Configuration

# Model Parameters
activation_dropout: 0.0
batch_size: 8
checkpoints: false
dropout: 0.1
embed_dim: 80
epochs: 1
ffn_dim: 12
fsdp: false
layers: 2
learning_rate: 0.001 # lr
model_type: retnet # Choices: "retnet", "transformer"
heads: 4
seq_len: 128
value_embed_dim: 12
vocab_size: 4000


# Dataset Configuration: Specifically Hugging Face
dataset_feature: "text"
dataset_name: "wikitext"
dataset_subset: "wikitext-2-v1"

#Download_Data
raw_dataset_path: "/Users/jtappen/301R/301r_retnet/data/datasets/wikitext"

#Train Tokenizer
tokenizer_path: "/Users/jtappen/301R/301r_retnet/data/tokenizers/wikitext_tokenizer"
tokenized_dataset_path: "/Users/jtappen/301R/301r_retnet/data/tokenized_dataset/wikitext"

#Tokenize Data
tokenized_data_name: "wikitext-2-v1-tokenized"

#Train Model
models_path: "/Users/jtappen/301R/301r_retnet/scripts/data/models"

splits:
  - 0.7
  - 0.2
  - 0.1

# Paths and Directories
data_directory: "/tmp/data" # data-dir
dataset_directory: "/tmp/data/dataset" # dataset-dir
tensorboard_directory: "/tmp/data/logs" # tboard-dir

# Device Configuration
device: "cuda" # Options: "cpu", "cuda"

# Training Configuration
rand_seed: 42 # rand-seed
validation_frequency: 3 # val-freq
